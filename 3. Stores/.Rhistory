install.packages("textir")
data(congress109)
library(textir)
data(congress109)
congress109Counts[c("Barack Obama","John Boehner"),995:998]
require("wordcloud")
install.packages("wordcloud")
require("wordcloud")
wordcloud(words = colnames(congress109Counts),
freq = colSums(congress109Counts),
min.freq = 100,
scale = c(3, 0.1), max.words=200,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
wordcloud(words = colnames(congress109Counts),
freq = colSums(congress109Counts),
min.freq = 1000,
scale = c(3, 0.1), max.words=30,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
f <- congress109Counts
y <- congress109Ideology$repshare
lassoslant <- cv.gamlr(congress109Counts>0, y)
B <- coef(lassoslant$gamlr)[-1,]
head(sort(round(B[B!=0],4)),10)
congress109Counts[c("Barack Obama","John Boehner"),995:998]
congress109Counts[c("Barack Obama","John Boehner"),995:998]
congress109Ideology[1:4,1:5]
wordcloud(words = colnames(congress109Counts), #palabras columnas
freq = colSums(congress109Counts), #frecuencia es la suma de esas palabras
min.freq = 100,
scale = c(3, 0.1), max.words=200,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1")) #set 1 es una paleta de colores determinada
tail(sort(round(B[B!=0],4)),10)
library(textir)
data(we8there)
x <- we8thereCounts
x[1,x[1,]!=0]
pca <- prcomp(x, scale=TRUE) # can take a long time
tail(sort(pca$rotation[,1]))
tail(sort(pca$rotation[,4]))
pca <- prcomp(x, scale=TRUE) # can take a long time
tail(sort(pca$rotation[,1]))
rm(list = ls())
setwd("C:/Users/amorales/OneDrive - ANI/Documentos/PM/USB/Andes - MEcA/Intersemestral/Big Data - Maching Lerning/Semana 7")
library(text2vec)
load('shakes words df 4text2vec.RData')
install.packages("text2vec")
load('shakes words df 4text2vec.RData')
load("~/PM/USB/Andes - MEcA/Intersemestral/Big Data - Maching Lerning/Semana 7/shakes_words_df_4text2vec.RData")
head(shakes words)
head(shakes_words)
shakes_vocab <- prune_vocabulary(shakes_vocab, term_count_min= 5)
shakes_words_ls <- list(shakes_words$word)
it <- itoken(shakes_words_ls, progressbar = FALSE)
it <- itoken(shakes_words_ls, progressbar = FALSE) #paso previo para crear el vocabulario
library(text2vec) #librería que permite organizar en vectores el texto
shakes_words_ls <- list(shakes_words$word)
it <- itoken(shakes_words_ls, progressbar = FALSE) #paso previo para crear el vocabulario
shakes_vocab <- create_vocabulary(it) #creo el vocabulario
head(shakes_vocab)
shakes_vocab_ls <- prune_vocabulary(shakes_vocab, term_count_min= 5)#me quedo con terminos que ocurran mas de 5 veces
head(shakes_vocab)
vectorizer <- vocab vectorizer(shakes vocab)
vectorizer <- vocab_vectorizer(shakes_vocab)
shakes_tcm <- create_tcm(it, vectorizer, skip_grams_window = 10)
glove <- GlobalVectors$new(rank = 50, x max = 10)
shakes wv main = glove$fit transform(shakes tcm, n iter = 10, convergence tol = 0.01, n threads = 8)
glove <- GlobalVectors$new(rank = 50, x_max = 10)
shakes_wv_main = glove$fit_transform(shakes_tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
dim(shakes wv main)
dim(shakes_wv_main)
shakes_wv_context <- glove$components
dim(shakes_wv_context)
shakes_word_vectors <- shakes_wv_main + t(shakes_wv_context) #sum U y V
rom <- shakes word vectors["romeo", , drop = F]
cos sim rom <- sim2(x =shakes word vectors, y = rom, method = "cosine", norm = "l2")
rom <- shakes_word_vectors["romeo", drop = F]
cos_sim_rom <- sim2(x =shakes_word_vectors, y = rom, method = "cosine", norm = "l2")
cos_sim_rom <- sim2(x =shakes_word_vectors, y = rom, method = "cosine", norm = "l2")
love <- shakes_word_vectors["love", drop = F]
love <- shakes_word_vectors["love", drop = F]
cos_sim_rom <- sim2(x <- shakes_word_vectors, y = love, method = "cosine", norm = "l2")
head(sort(cos_sim_rom[,1], decreasing <- T), 10)
test <- shakes_word_vectors["romeo", , drop = F] -
shakes_word_vectors["mercutio", , drop = F] +
shakes_word_vectors["nurse", , drop = F]
cos_sim_test <- sim2(x = shakes_word_vectors, y = test, method = "cosine", norm = "l2")
head(sort(cos_sim_test[,1], decreasing = T), 10)
test <- shakes_word_vectors["romeo", , drop = F] -
shakes_word_vectors["juliet", , drop = F] +
shakes_word_vectors["cleopatra", , drop = F]
cos_sim_test <- sim2(x = shakes_word_vectors, y = test, method = "cosine", norm = "l2")
head(sort(cos_sim_test[,1], decreasing = T), 3)
rm(list = ls())
setwd("C:/Users/andre/Downloads")
setwd("C:/Users/amorales/OneDrive - ANI/Documentos/GitHub/Problem_Set_3_G16/3. Stores")
require(pacman)
p_load(tidyverse,
rvest,
writexl,
rio,
skimr,
sf,
pastecs,
stargazer,
PerformanceAnalytics,
naniar,
gtsummary
)
df_hogares<- readRDS("df_house_mnz.rds")
st_geometry(df_hogares) = NULL
df_hogares <- df_hogares %>% mutate(base_Neighborhood = paste0(base, " ", Neighborhood))
table(df_hogares$base_Neighborhood)
table(df_hogares$Neighborhood == "Poblado")
sum(df_hogares$Neighborhood)
table(is.na(df_hogares$Neighborhood))
summary(is.na(df_hogares$Neighborhood==T))
df_hogares$Neighborhood
df_trainCH <- df_hogares %>% subset(base == "train" & Neighborhood == "Chapinero")
df_trainP <- df_hogares %>% subset(base == "train" & Neighborhood == "Poblado")
df_testCH <- df_hogares %>% subset(base == "test" & Neighborhood == "Chapinero")
df_testP <- df_hogares %>% subset(base == "test" & Neighborhood == "Poblado")
summary(df_trainCH)
summary(df_trainP)
summary(df_testCH)
summary(df_testP)
df_hogares1 <- df_hogares %>% select(bedrooms, bathrooms, surface_total, price, property_type, dist_bar, dist_bus_station, dist_bank, dist_restaurante, dist_school, dist_park, parking, ascensor, balcon, terraza, remodelado, estrato, base_Neighborhood)
df_hogares1 <- df_hogares %>% select(bedrooms, bathrooms, surface_total, price, property_type, dist_bar, dist_bus_station, dist_bank, dist_restaurant, dist_school, dist_park, parking, ascensor, balcon, terraza, remodelado, estrato, base_Neighborhood)
table1 <- tbl_summary(df_hogares1)
table1
tbl_summary(df_hogares1, by= base_Neighborhood, statistic = list (all_continuous()~"{mean} ({sd})")) # por clasificación
R.version.string
p_load(tidyverse,
rvest,
writexl,
rio,
skimr,
sf,
pastecs,
stargazer,
PerformanceAnalytics,
naniar,
gtsummary,
stringr,
rgeos,
plotly,
leaflet,
tmaptools,
osmdata
)
